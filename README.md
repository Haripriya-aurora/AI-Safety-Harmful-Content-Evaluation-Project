# AI-Safety-Harmful-Content-Evaluation-Project
Evaluated model responses to harmful, unsafe, and policy-sensitive prompts. Scored outputs using a safety-focused rubric covering refusal quality, empathy, de-escalation, and policy alignment. Built a structured dataset simulating real AI safety and RLHF evaluation workflows.
